#!groovy



pipeline {
  options {
    disableConcurrentBuilds()
    buildDiscarder(logRotator(numToKeepStr: '5'))
  }
  parameters {
    credentials(name: 'jenkins_api_creds', description: 'ID of credentials containing API username/key for submitting Jenkins API calls (used when adding/running/deleting CI jobs)', defaultValue: '', credentialType: 'com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl', required: true )
    credentials(name: 'git_auth_key_creds', description: 'ID of credentials containing GitHub OAuth Token that will allow for cloning repositories from JPL GitHub.', defaultValue: '', credentialType: 'com.cloudbees.plugins.credentials.impl.StringCredentialsImpl', required: true )
  }
  agent none
  stages{
    stage("Generate unique counter"){
      agent any
      steps{
        writeFile file: "the_job_name", text: """${JOB_NAME}"""
        script {
          jobNameHash = sha1 "the_job_name"
          counter = jobNameHash[0..3]
          name = currentBuild.displayName
          currentBuild.displayName = "opera-dv-${counter}-pcm ${name}"
        }
      }
    }
    stage("Gather Cluster Configuration"){
      input {
        message "Provide Cluster Configuration"
        ok "Start Provisioning"
        parameters {
          string(name: 'hysds_release', defaultValue: 'develop', description: 'Version of HySDS to install in cluster.')
          string(name: 'subnet_id', defaultValue: 'subnet-8ecc5dd3', description: 'ID of the subnet PCM components will run in.')
          string(name: 'verdi_security_group_id', defaultValue: 'sg-06adb1e6e9510c99c', description: 'ID of the security group used for Verdi workers.')
          string(name: 'cluster_security_group_id', defaultValue: 'sg-026af415158b38796', description: 'ID of the security group used for PCM.')
          string(name: 'asg_vpc', defaultValue: 'vpc-9b3500ff', description: 'ID of the VPC used for the workers.')
          string(name: 'asg_ami', defaultValue: 'ami-0c6480025b7828f15', description: 'AMI ID used for the workers.')
          string(name: 'pcm_branch', defaultValue: 'develop', description: 'Branch of https://github.jpl.nasa.gov/IEMS-SDS/opera-pcm to checkout on cluster.')
          string(name: 'baseline_pge_branch', defaultValue: 'develop', description: 'Branch of https://github.jpl.nasa.gov/IEMS-SDS/baseline-pge that will be used for submitting jobs.')
          string(name: 'product_delivery_branch', defaultValue: 'develop', description: 'Branch of https://github.jpl.nasa.gov/IEMS-SDS/CNM_product_delivery that will be used to send CNM Notification messages.')
          string(name: 'cnm_r_event_trigger', defaultValue: 'sns', description: 'Specify the event trigger to handle CNM-R messages. Valid values are "kinesis" or "sns".')
          string(name: 'lambda_package_release', defaultValue: 'develop', description: 'Specify the lambda package release.')
        }
      }
      steps{
        node(''){
          script {
            tf_vars = [
            "hysds_release":"${hysds_release}",
            "subnet_id":"${subnet_id}",
            "verdi_security_group_id":"${verdi_security_group_id}",
            "cluster_security_group_id":"${cluster_security_group_id}",
            "asg_vpc":"${asg_vpc}",
            "asg_ami":"${asg_ami}",
            "counter":"${counter}",
            "baseline_pge_branch":"${baseline_pge_branch}",
            "product_delivery_branch":"${product_delivery_branch}",
            "cnm_r_event_trigger":"${cnm_r_event_trigger}",
            "lambda_package_release":"${lambda_package_release}"
            ]
          }
          writeFile file: "death-valley.auto.tfvars", text: """\
          hysds_release="${hysds_release}"
          subnet_id="${subnet_id}"
          verdi_security_group_id="${verdi_security_group_id}"
          cluster_security_group_id="${cluster_security_group_id}"
          asg_vpc="${asg_vpc}"
          asg_ami="${asg_ami}"
          counter="${counter}"
          pcm_branch="${pcm_branch}"
          baseline_pge_branch="${baseline_pge_branch}"
          product_delivery_branch="${product_delivery_branch}"
          cnm_r_event_trigger="${cnm_r_event_trigger}"
          lambda_package_release="${lambda_package_release}"
          """.stripIndent()
          stash includes: 'death-valley.auto.tfvars', name: 'death-valley_auto_tfvars'
        }
      }
    }
    stage("Provision Death Valley Cluster"){
      options { timestamps() }
      agent any
      steps{
        dir("tfworkingdir"){
          unstash name: 'death-valley_auto_tfvars'
          script{
            if(!fileExists('death-valley.auto.tfvars')){
              error("death-valley.auto.tfvars file not found")
            }
            tf_vars = readProperties(file:'death-valley.auto.tfvars')
          }
          deleteDir()
        }
        dir("tfworkingdir"){
          sh label: "Copy Terraform Modules", script: """
          cp -r ${WORKSPACE}/cluster_provisioning/death-valley .
          cp -r ${WORKSPACE}/cluster_provisioning/modules .
          """
          dir("death-valley"){
            sh label: "Init Terraform", script: """
            /home/hysdsops/bin/terraform init \
            -upgrade -force-copy \
            -backend-config="bucket=opera-pcm-code-bucket-dv" \
            -backend-config="key=death-valley-tfstate/opera-dv-${tf_vars.counter}-pcm" \
            -backend-config="region=us-west-2"
            """
            unstash name: 'death-valley_auto_tfvars'
            withCredentials([sshUserPrivateKey(credentialsId: 'pcmdev', keyFileVariable: 'identityFile', passphraseVariable: '', usernameVariable: 'userName')]) {
            withCredentials([usernamePassword(credentialsId: params.jenkins_api_creds, usernameVariable: 'jenkins_api_user', passwordVariable: 'jenkins_api_key')]) {
            withCredentials([string(credentialsId: params.git_auth_key_creds, variable: 'git_auth_key')]) {
              sh label: "Validate Terraform", script: """
              /home/hysdsops/bin/terraform validate --var private_key_file=${identityFile} \
              --var jenkins_api_user=${jenkins_api_user} --var jenkins_api_key=${jenkins_api_key} \
              --var git_auth_key=${git_auth_key} --var keypair_name=pcmdev \
              --var pcm_branch=${tf_vars.pcm_branch} --var hysds_release=${tf_vars.hysds_release} \
              --var product_delivery_branch=${tf_vars.product_delivery_branch} --var cnm_r_event_trigger=${tf_vars.cnm_r_event_trigger} \
              --var lambda_package_release=${tf_vars.lambda_package_release}
              """
              sh label: "Apply Terraform", script: """
              /home/hysdsops/bin/terraform apply --var private_key_file=${identityFile} \
              --var jenkins_api_user=${jenkins_api_user} --var jenkins_api_key=${jenkins_api_key} \
              --var git_auth_key=${git_auth_key} --var keypair_name=pcmdev \
              --var pcm_branch=${tf_vars.pcm_branch} --var hysds_release=${tf_vars.hysds_release} \
              --var product_delivery_branch=${tf_vars.product_delivery_branch} --var cnm_r_event_trigger=${tf_vars.cnm_r_event_trigger} \
              --var lambda_package_release=${tf_vars.lambda_package_release} \
              -auto-approve
              """
            }}}

            script {
              tf_output = [
              "small_asg_name":sh (script: '/home/hysdsops/bin/terraform output small_asg_name', returnStdout: true).trim(),
              "small_queue_name":sh (script: '/home/hysdsops/bin/terraform output small_queue_name', returnStdout: true).trim(),
              "large_asg_name":sh (script: '/home/hysdsops/bin/terraform output large_asg_name', returnStdout: true).trim(),
              "large_queue_name":sh (script: '/home/hysdsops/bin/terraform output large_queue_name', returnStdout: true).trim(),
              "cnm_notify_asg_name":sh (script: '/home/hysdsops/bin/terraform output cnm_notify_asg_name', returnStdout: true).trim(),
              "cnm_notify_queue_name":sh (script: '/home/hysdsops/bin/terraform output cnm_notify_queue_name', returnStdout: true).trim(),
              "full_venue":sh (script: '/home/hysdsops/bin/terraform output full_venue', returnStdout: true).trim(),
              "dataset_bucket":sh (script: '/home/hysdsops/bin/terraform output dataset_bucket', returnStdout: true).trim(),
              "code_bucket":sh (script: '/home/hysdsops/bin/terraform output code_bucket', returnStdout: true).trim(),
              "triage_bucket":sh (script: '/home/hysdsops/bin/terraform output triage_bucket', returnStdout: true).trim(),
              "lts_bucket":sh (script: '/home/hysdsops/bin/terraform output lts_bucket', returnStdout: true).trim(),
              "mozart_pvt_ip":sh (script: '/home/hysdsops/bin/terraform output mozart_pvt_ip', returnStdout: true).trim()
              ]

              writeFile file: "tf_output", text: """\
              small_asg_name=${tf_output.small_asg_name}
              small_queue_name=${tf_output.small_queue_name}
              large_asg_name=${tf_output.large_asg_name}
              large_queue_name=${tf_output.large_queue_name}
              cnm_notify_asg_name=${tf_output.cnm_notify_asg_name}
              cnm_notify_queue_name=${tf_output.cnm_notify_queue_name}
              full_venue=${tf_output.full_venue}
              dataset_bucket=${tf_output.dataset_bucket}
              code_bucket=${tf_output.code_bucket}
              triage_bucket=${tf_output.triage_bucket}
              lts_bucket=${tf_output.lts_bucket}
              mozart_pvt_ip=${tf_output.mozart_pvt_ip}
              """.stripIndent()
              stash includes: 'tf_output', name: 'tf_output'
            }
          }
        }

      }
    }
    stage("Build Baseline PGE"){
      agent any
      options {
        timestamps()
      }
      steps{
        dir("tempload"){
          unstash name: 'death-valley_auto_tfvars'
          script{
            if(!fileExists('death-valley.auto.tfvars')){
              error("death-valley.auto.tfvars file not found")
            }
            tf_vars = readProperties(file:'death-valley.auto.tfvars')
          }
          deleteDir()
        }
        withCredentials([sshUserPrivateKey(credentialsId: 'pcmdev', keyFileVariable: 'identityFile', passphraseVariable: '', usernameVariable: 'userName')]) {
          sshCommand (
            remote: ['name': 'mozart', 'host': tf_output.mozart_pvt_ip, 'user': userName, 'identityFile': identityFile, 'allowAnyHosts': true],
            command: "/export/home/hysdsops/mozart/ops/opera-pcm/cluster_provisioning/death-valley/scripts/build_baseline_pge.sh ${tf_vars.baseline_pge_branch}")
        }
      }
    }

    stage("Submit PGE Jobs"){
      options { timestamps() }
      input {
        message "How many jobs should be submitted?"
        ok "Submit Jobs"
        parameters {
          string(name: 'num_jobs', defaultValue: '', description: 'Number of jobs to submit to mozart.')
          string(name: 'min_sleep', defaultValue: '30', description: 'Minimum time in seconds jobs will simulate CPU work.')
          string(name: 'max_sleep', defaultValue: '60', description: 'Maximum time in seconds jobs will simulate CPU work.')
        }
      }
      steps{
        node(''){
          dir("tempload"){
            unstash name: 'death-valley_auto_tfvars'
            script{
              if(!fileExists('death-valley.auto.tfvars')){
                error("death-valley.auto.tfvars file not found")
              }
              tf_vars = readProperties(file:'death-valley.auto.tfvars')
            }
            deleteDir()
          }
          unstash name: 'tf_output'
          script{
            if(!fileExists('tf_output')){
              error("tf_output file not found")
            }
            tf_output = readProperties(file:'tf_output')
          }
          withCredentials([sshUserPrivateKey(credentialsId: 'pcmdev', keyFileVariable: 'identityFile', passphraseVariable: '', usernameVariable: 'userName')]) {
          withCredentials([string(credentialsId: params.git_auth_key_creds, variable: 'git_auth_key')]) {
            sshCommand (
              remote: ['name': 'mozart', 'host': tf_output.mozart_pvt_ip, 'user': userName, 'identityFile': identityFile, 'allowAnyHosts': true],
              command: "/export/home/hysdsops/mozart/ops/opera-pcm/cluster_provisioning/death-valley/scripts/submit_pge_jobs.sh ${num_jobs} ${tf_vars.baseline_pge_branch} ${tf_output.small_queue_name} ${min_sleep} ${max_sleep} ${git_auth_key}")
          }}
        }
      }
    }

    stage("Allow ASG to Scale"){
      options { timestamps() }
      input {
        message "Are you sure you want to scale?"
        ok "Commence Death Valley Immediately"
        parameters {
          string(name: 'max_workers', defaultValue: '3000', description: 'Max number of workers allowed to scale up to.')
        }
      }
      steps{
        node(''){
          unstash name: 'tf_output'
          script{
            if(!fileExists('tf_output')){
              error("tf_output file not found")
            }
            tf_output = readProperties(file:'tf_output')
          }
          withCredentials([sshUserPrivateKey(credentialsId: 'pcmdev', keyFileVariable: 'identityFile', passphraseVariable: '', usernameVariable: 'userName')]) {
            sshCommand (
              remote: ['name': 'mozart', 'host': tf_output.mozart_pvt_ip, 'user': userName, 'identityFile': identityFile, 'allowAnyHosts': true],
              command: "/export/home/hysdsops/mozart/ops/opera-pcm/cluster_provisioning/death-valley/scripts/increase_max_scale_asg.sh ${tf_output.small_asg_name} ${tf_output.cnm_notify_asg_name} ${max_workers}"
            )
          }
        }
      }
    }

    stage("Destroy Cluster"){
      options { timestamps() }
      input {
        message "Destroy cluster immediately?"
        ok "Remove all resources"
      }
      steps{
        node(''){
          unstash name: 'tf_output'
          script{
            if(!fileExists('tf_output')){
              error("tf_output file not found")
            }
            tf_output = readProperties(file:'tf_output')
          }
          dir("tfworkingdir"){
            unstash name: 'death-valley_auto_tfvars'
            script{
              if(!fileExists('death-valley.auto.tfvars')){
                error("death-valley.auto.tfvars file not found")
              }
              tf_vars = readProperties(file:'death-valley.auto.tfvars')
            }
            deleteDir()
          }
          dir("tfworkingdir"){
            sh label: "Copy Terraform Modules", script: """
            cp -r ${WORKSPACE}/cluster_provisioning/death-valley .
            cp -r ${WORKSPACE}/cluster_provisioning/modules .
            """
            dir("death-valley"){
              sh label: "Init Terraform", script: """
              /home/hysdsops/bin/terraform init \
              -upgrade -force-copy \
              -backend-config="bucket=opera-pcm-code-bucket-dv" \
              -backend-config="key=death-valley-tfstate/opera-dv-${tf_vars.counter}-pcm" \
              -backend-config="region=us-west-2"
              """
              unstash name: 'death-valley_auto_tfvars'
              withCredentials([sshUserPrivateKey(credentialsId: 'pcmdev', keyFileVariable: 'identityFile', passphraseVariable: '', usernameVariable: 'userName')]) {
              withCredentials([usernamePassword(credentialsId: params.jenkins_api_creds, usernameVariable: 'jenkins_api_user', passwordVariable: 'jenkins_api_key')]) {
              withCredentials([string(credentialsId: params.git_auth_key_creds, variable: 'git_auth_key')]) {
                sshCommand (
                  remote: ['name': 'mozart', 'host': tf_output.mozart_pvt_ip, 'user': userName, 'identityFile': identityFile, 'allowAnyHosts': true],
                  command: "/export/home/hysdsops/mozart/ops/opera-pcm/cluster_provisioning/death-valley/scripts/purge_aws_resources.sh ${tf_output.code_bucket} ${tf_output.dataset_bucket} ${tf_output.triage_bucket} ${tf_output.lts_bucket} ${tf_output.small_asg_name} ${tf_output.large_asg_name} ${tf_output.cnm_notify_asg_name} ${tf_output.full_venue} ${tf_vars.cnm_r_event_trigger}")
                sh label: "Destroy Terraform Resources", script: """
                /home/hysdsops/bin/terraform destroy --var private_key_file=${identityFile} \
                --var jenkins_api_user=${jenkins_api_user} --var jenkins_api_key=${jenkins_api_key} \
                --var git_auth_key=${git_auth_key} --var keypair_name=pcmdev \
                --var pcm_branch=${tf_vars.pcm_branch} --var hysds_release=${tf_vars.hysds_release} \
                --var product_delivery_branch=${tf_vars.product_delivery_branch} --var cnm_r_event_trigger=${tf_vars.cnm_r_event_trigger} \
                --var lambda_package_release=${tf_vars.lambda_package_release} \
                -auto-approve
                """
              }}}
            }
          }
        }
      }
    }
  }
}
